{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d58f5-0c97-47eb-ba67-f9e02345c3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0996eb-328e-41ea-8e3d-0a7ea4dd73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tensorflow.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "dataset_dir = \"D:\\\\face_age\"\n",
    "images = []\n",
    "labels = []\n",
    "for age_folder in os.listdir(dataset_dir):\n",
    "    if os.path.isdir(os.path.join(dataset_dir, age_folder)):\n",
    "        age = int(age_folder)\n",
    "        for image_file in os.listdir(os.path.join(dataset_dir, age_folder)):\n",
    "            img_path = os.path.join(dataset_dir, age_folder, image_file)\n",
    "            try:\n",
    "                img = load_img(img_path, target_size=(224, 224))\n",
    "                img_array = img_to_array(img)\n",
    "                img_array = preprocess_input(img_array) \n",
    "                images.append(img_array)\n",
    "                labels.append(age)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image '{img_path}': {e}\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25d9f0-b6b8-4b6c-96d1-89914ca6fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAEStopCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(MAEStopCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('mae') <= self.threshold:\n",
    "            print(f\"\\nStopping training as MAE reached {self.threshold}\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29777e52-1f32-4dc1-ab84-7a03312774ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation=None)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "callback = MAEStopCallback(threshold=1.0)\n",
    "model.fit(images, labels, epochs=50, batch_size=32, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01271e79-aab6-4a40-a0fb-3a7458880ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_path = \"D:\\\\face_age\\\\075\\\\2108.png\"\n",
    "new_img = load_img(new_img_path, target_size=(224, 224))\n",
    "new_img_array = img_to_array(new_img)\n",
    "new_img_array = np.expand_dims(new_img_array, axis=0)\n",
    "new_img_array = preprocess_input(new_img_array)  \n",
    "\n",
    "prediction = model.predict(new_img_array)\n",
    "predicted_age = int(np.round(prediction[0][0]))\n",
    "\n",
    "print(\"Predicted Age:\", predicted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1389e-c652-4d85-b84d-88fcfb49a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_age(face_img):\n",
    "    face_img = cv2.resize(face_img, (224, 224))\n",
    "    face_img = img_to_array(face_img)\n",
    "    face_img = np.expand_dims(face_img, axis=0)\n",
    "    face_img = preprocess_input(face_img)\n",
    "    prediction = model.predict(face_img)\n",
    "    predicted_age = int(np.round(prediction[0][0])) \n",
    "\n",
    "    return predicted_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b9087-4380-4f07-b170-c6795f54a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        predicted_age = predict_age(face_img)\n",
    "        if predicted_age >= 18:\n",
    "            message = f\"Age: {predicted_age}. Access Given\"\n",
    "        else:\n",
    "            message = f\"Age: {predicted_age}. Access Denied\"\n",
    "        cv2.putText(frame, message, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow('Face Age Estimation', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35edff-d97f-4f50-b765-25a53e44f7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
